"""
ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì„œë¹„ìŠ¤
í™ˆí˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„
"""

import re
import time
from typing import Dict, List, Optional
from urllib.parse import urlparse, urljoin
import requests
from bs4 import BeautifulSoup

class WebScraper:
    """ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.timeout = 15
        self.max_retries = 3
        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    
    def scrape_website(self, url: str) -> Dict:
        """
        ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘
        
        Args:
            url: ì›¹ì‚¬ì´íŠ¸ URL
            
        Returns:
            Dict: ìˆ˜ì§‘ëœ ì›¹ì‚¬ì´íŠ¸ ì •ë³´
        """
        try:
            print(f"ğŸŒ {url} ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
            
            # ì›¹í˜ì´ì§€ ë‚´ìš© ìˆ˜ì§‘
            content = self._fetch_webpage(url)
            if not content:
                return self._get_sample_website_data(url)
            
            # ì •ë³´ ì¶”ì¶œ
            website_info = {
                'url': url,
                'title': self._extract_title(content),
                'description': self._extract_description(content),
                'keywords': self._extract_keywords(content),
                'company_info': self._extract_company_info(content),
                'contact_info': self._extract_contact_info(content),
                'social_links': self._extract_social_links(content),
                'last_updated': self._get_current_date(),
                'status': 'success'
            }
            
            print("âœ… ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
            return website_info
            
        except Exception as e:
            print(f"âŒ ì›¹ì‚¬ì´íŠ¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._get_sample_website_data(url)
    
    def _fetch_webpage(self, url: str) -> Optional[BeautifulSoup]:
        """ì›¹í˜ì´ì§€ ë‚´ìš© ìˆ˜ì§‘"""
        for attempt in range(self.max_retries):
            try:
                headers = {
                    'User-Agent': self.user_agent,
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                }
                
                response = requests.get(url, headers=headers, timeout=self.timeout)
                response.raise_for_status()
                
                # ì¸ì½”ë”© ê°ì§€
                if response.encoding.lower() in ['iso-8859-1', 'windows-1252']:
                    response.encoding = response.apparent_encoding
                
                soup = BeautifulSoup(response.content, 'html.parser')
                return soup
                
            except Exception as e:
                print(f"âš ï¸ ì›¹í˜ì´ì§€ ìˆ˜ì§‘ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    return None
        
        return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        try:
            # title íƒœê·¸ì—ì„œ ì¶”ì¶œ
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()
                # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°
                if len(title) > 100:
                    title = title[:100] + "..."
                return title
            
            # h1 íƒœê·¸ì—ì„œ ì¶”ì¶œ
            h1_tag = soup.find('h1')
            if h1_tag:
                return h1_tag.get_text().strip()[:100]
            
            return "ì œëª© ì—†ìŒ"
            
        except Exception:
            return "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """í˜ì´ì§€ ì„¤ëª… ì¶”ì¶œ"""
        try:
            # meta descriptionì—ì„œ ì¶”ì¶œ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                return meta_desc.get('content').strip()[:200]
            
            # ì²« ë²ˆì§¸ p íƒœê·¸ì—ì„œ ì¶”ì¶œ
            first_p = soup.find('p')
            if first_p:
                desc = first_p.get_text().strip()
                if len(desc) > 200:
                    desc = desc[:200] + "..."
                return desc
            
            return "ì„¤ëª… ì—†ìŒ"
            
        except Exception:
            return "ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨"
    
    def _extract_keywords(self, soup: BeautifulSoup) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            keywords = []
            
            # meta keywordsì—ì„œ ì¶”ì¶œ
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            if meta_keywords and meta_keywords.get('content'):
                keywords_text = meta_keywords.get('content')
                keywords.extend([kw.strip() for kw in keywords_text.split(',') if kw.strip()])
            
            # h1, h2, h3 íƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            for tag in soup.find_all(['h1', 'h2', 'h3']):
                text = tag.get_text().strip()
                if text and len(text) < 50:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸
                    keywords.append(text)
            
            # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
            unique_keywords = list(dict.fromkeys(keywords))[:10]
            return unique_keywords
            
        except Exception:
            return []
    
    def _extract_company_info(self, soup: BeautifulSoup) -> Dict:
        """íšŒì‚¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            company_info = {}
            text = soup.get_text()
            
            # íšŒì‚¬ëª… íŒ¨í„´ ê²€ìƒ‰
            company_patterns = [
                r'([ê°€-í£A-Za-z&Â·ã†\-\s]{2,}?)\s*(ì£¼ì‹íšŒì‚¬|ãˆœ)',
                r'(ì£¼ì‹íšŒì‚¬|ãˆœ)\s*([ê°€-í£A-Za-z&Â·ã†\-\s]{2,})',
                r'([ê°€-í£A-Za-z&Â·ã†\-\s]{2,}?)\s*(ìœ í•œíšŒì‚¬|\(ìœ \))',
                r'(ìœ í•œíšŒì‚¬|\(ìœ \))\s*([ê°€-í£A-Za-z&Â·ã†\-\s]{2,})'
            ]
            
            for pattern in company_patterns:
                matches = re.findall(pattern, text)
                if matches:
                    company_info['legal_name'] = matches[0][0] if matches[0][0] else matches[0][1]
                    break
            
            # ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸ ê²€ìƒ‰
            biz_pattern = r'ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸\s*:?\s*(\d{3}-\d{2}-\d{5})'
            biz_match = re.search(biz_pattern, text)
            if biz_match:
                company_info['business_number'] = biz_match.group(1)
            
            # ëŒ€í‘œìëª… ê²€ìƒ‰
            ceo_patterns = [
                r'ëŒ€í‘œì\s*:?\s*([ê°€-í£]{2,4})',
                r'ëŒ€í‘œì´ì‚¬\s*:?\s*([ê°€-í£]{2,4})',
                r'CEO\s*:?\s*([ê°€-í£A-Za-z\s]{2,20})'
            ]
            
            for pattern in ceo_patterns:
                ceo_match = re.search(pattern, text)
                if ceo_match:
                    company_info['ceo'] = ceo_match.group(1).strip()
                    break
            
            return company_info
            
        except Exception:
            return {}
    
    def _extract_contact_info(self, soup: BeautifulSoup) -> Dict:
        """ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            contact_info = {}
            text = soup.get_text()
            
            # ì „í™”ë²ˆí˜¸ íŒ¨í„´
            phone_patterns = [
                r'(\d{2,3}-\d{3,4}-\d{4})',
                r'(\d{2,3}\s\d{3,4}\s\d{4})',
                r'(\+82\s?\d{2,3}\s?\d{3,4}\s?\d{4})'
            ]
            
            phones = []
            for pattern in phone_patterns:
                matches = re.findall(pattern, text)
                phones.extend(matches)
            
            if phones:
                contact_info['phones'] = list(set(phones))[:3]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 3ê°œ
            
            # ì´ë©”ì¼ íŒ¨í„´
            email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
            emails = re.findall(email_pattern, text)
            if emails:
                contact_info['emails'] = list(set(emails))[:3]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 3ê°œ
            
            # ì£¼ì†Œ íŒ¨í„´ (ê°„ë‹¨í•œ íŒ¨í„´)
            address_patterns = [
                r'([ê°€-í£]{2,4}(?:ì‹œ|ë„)\s[ê°€-í£]{2,4}(?:êµ¬|êµ°|ì‹œ)\s[ê°€-í£\s\d\-]+)',
                r'([ê°€-í£]{2,4}(?:êµ¬|êµ°|ì‹œ)\s[ê°€-í£\s\d\-]+)'
            ]
            
            addresses = []
            for pattern in address_patterns:
                matches = re.findall(pattern, text)
                addresses.extend(matches)
            
            if addresses:
                contact_info['addresses'] = list(set(addresses))[:2]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 2ê°œ
            
            return contact_info
            
        except Exception:
            return {}
    
    def _extract_social_links(self, soup: BeautifulSoup) -> List[str]:
        """ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì¶”ì¶œ"""
        try:
            social_links = []
            social_patterns = {
                'facebook': r'facebook\.com/[^/\s]+',
                'twitter': r'twitter\.com/[^/\s]+',
                'instagram': r'instagram\.com/[^/\s]+',
                'linkedin': r'linkedin\.com/[^/\s]+',
                'youtube': r'youtube\.com/[^/\s]+'
            }
            
            # ëª¨ë“  ë§í¬ ê²€ìƒ‰
            for link in soup.find_all('a', href=True):
                href = link['href']
                for platform, pattern in social_patterns.items():
                    if re.search(pattern, href):
                        social_links.append(href)
                        break
            
            return list(set(social_links))[:5]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 5ê°œ
            
        except Exception:
            return []
    
    def _get_sample_website_data(self, url: str) -> Dict:
        """ìƒ˜í”Œ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ìƒì„±"""
        return {
            'url': url,
            'title': 'ìƒ˜í”Œ ì›¹ì‚¬ì´íŠ¸',
            'description': 'ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'keywords': ['ìƒ˜í”Œ', 'ì›¹ì‚¬ì´íŠ¸'],
            'company_info': {
                'legal_name': 'ìƒ˜í”Œ íšŒì‚¬',
                'business_number': '123-45-67890',
                'ceo': 'í™ê¸¸ë™'
            },
            'contact_info': {
                'phones': ['02-1234-5678'],
                'emails': ['info@example.com'],
                'addresses': ['ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬']
            },
            'social_links': [],
            'last_updated': self._get_current_date(),
            'status': 'error'
        }
    
    def _get_current_date(self) -> str:
        """í˜„ì¬ ë‚ ì§œ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
